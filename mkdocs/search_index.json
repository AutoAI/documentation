{
    "docs": [
        {
            "location": "/", 
            "text": "Technical Docs\n\n\nThis is where we document all of our previous work, current projects, and projections for the future. Here you will find detailed technical specifications for all of our past and current vehicles.\n\n\nWho We Are\n\n\nWe develop open source software for autonomous vehicles so anyone can build one. \n\n\nCurrent Tasks\n\n\n\n\nBuilding out our knowledge base\n\n\nBuilding the \nBirdie\n\n\nLearning how to develop ROS Modules\n\n\n\n\nGetting Started\n\n\nIn order to get started, you'll need to become familiar with the following languages and services:\n\n\n\n\nC/C++\n, as all our production code is written in C/C++\n\n\nPython\n, for rapid prototyping\n\n\nROS\n, which ties all of our systems together\n\n\nLinux\n, specifically Ubuntu, which is used by all of our members for developing software. Our production code runs on Ubuntu 14.04 LTS\n\n\nGit\n, knowledge of which is essential. We use GitHub to manage our codebase and keep everything up to date and organized. We look forward to your pull requests!\n\n\n\n\nThe DriveAI Knowledge Base\n\n\nRead through our knowledge base \nhere\n to learn what we know so far! Getting up to speed on our current knowledge base is the fastest way to get to the front lines of development.\n\n\nGet Involved!\n\n\nWe use Slack as our primary communications interface. When you're comfortable with everything listed above and want to get involved with the team, send a message to \ncommunity@driveai.org\n and someone will invite you to our Slack team. That's all there is to it!\n\n\nCompleted Vehicles\n\n\n\n\nCockroach", 
            "title": "DriveAI"
        }, 
        {
            "location": "/#technical-docs", 
            "text": "This is where we document all of our previous work, current projects, and projections for the future. Here you will find detailed technical specifications for all of our past and current vehicles.", 
            "title": "Technical Docs"
        }, 
        {
            "location": "/#who-we-are", 
            "text": "We develop open source software for autonomous vehicles so anyone can build one.", 
            "title": "Who We Are"
        }, 
        {
            "location": "/#current-tasks", 
            "text": "Building out our knowledge base  Building the  Birdie  Learning how to develop ROS Modules", 
            "title": "Current Tasks"
        }, 
        {
            "location": "/#getting-started", 
            "text": "In order to get started, you'll need to become familiar with the following languages and services:   C/C++ , as all our production code is written in C/C++  Python , for rapid prototyping  ROS , which ties all of our systems together  Linux , specifically Ubuntu, which is used by all of our members for developing software. Our production code runs on Ubuntu 14.04 LTS  Git , knowledge of which is essential. We use GitHub to manage our codebase and keep everything up to date and organized. We look forward to your pull requests!", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#the-driveai-knowledge-base", 
            "text": "Read through our knowledge base  here  to learn what we know so far! Getting up to speed on our current knowledge base is the fastest way to get to the front lines of development.", 
            "title": "The DriveAI Knowledge Base"
        }, 
        {
            "location": "/#get-involved", 
            "text": "We use Slack as our primary communications interface. When you're comfortable with everything listed above and want to get involved with the team, send a message to  community@driveai.org  and someone will invite you to our Slack team. That's all there is to it!", 
            "title": "Get Involved!"
        }, 
        {
            "location": "/#completed-vehicles", 
            "text": "Cockroach", 
            "title": "Completed Vehicles"
        }, 
        {
            "location": "/knowledge/", 
            "text": "Introduction\n\n\nThis is where everything we know about self driving vehicles will be found. We're eager to have people join, learn from, and contribute to our project.\n\n\nWhat are they?\n\n\nAutonomous vehicles (AV's), also known as self-driving cars, are vehicles that drive themselves. For now, the vehicles are generally required to be supervised by a driver, but the goal is for the cars to be able to drive themselves on their own.\n\n\nWhy are they important?\n\n\nAutonomous vehicles will have an impact similar to the internet in magnitude and nature. The internet allowed us to move information at a reduced cost and without human interaction. Autonomous vehicles will have the same effect on the transportation of physical goods.\n\n\nMore importantly, autonomous vehicles are going to save a tremendous amount of lives. Driver error is a leading cause of death, and self-driving vehicles will make the roads safer because they aren't prone to making the same types of errors.\n\n\nHow do they work?\n\n\nAutonomous vehicles use an array of sensors, including GPS, cameras, and radar, to gather information about the world around them (more about sensors \nhere\n). Data from these sensors is then processed by state of the art algorithms to interpret the world in a meaningful way. They then use this interpretation to make fast and reliable decisions.\n\n\nChapter 1: Levels of autonomy\n\n\nAccording to the \nNational Highway Traffic Safety Administration\n, there are 5 levels of autonomy (including Level 0: No Automation). We've shortened their definitions for brevity, but the full definitions can be found \nhere\n.\n\n\n\n\nLevel 0 - No Automation:\n The driver is soley resposible for monitoring the roadway, and for safe operation of all vehicle controls.\n\n\nLevel 1 - Function-specific Automation:\n The driver can choose to cede limited authoiry over a primary control (as in adaptive cruise control), the vehicle can automatically assume limmited authority over a primary control (as in electronic stability control), or the automated system can provide added control to aid the driver in certain normal driving or crash-imminent situations (e.g., dynamic brake support in emergencies).\n\n\nLevel 2 - Combined Function Automation:\n The driver is still responsible for monitoring the roadway and safe opperation and is expected to be available for control at all times and on short notice. At level 2 in the specefic operation conditions for which the system is designed, an automated operating mode is enabled such that the driver is disengaged from physically operating the vehicle by having his or her hands off the steering wheel and foot off the pedal at the same time.\n\n\nLevel 3 - Limited Self-Driving Automation:\n Vehicles at this level of automation enable the driver to cede full control of all safety-critical functions under certain traffic or environmental conditions and in those conditions to rely heavily on the vehicle to monitor for changes in those conditions requiring transition back to driver control. The driver is expected to be available for occasional control, but with sufficiently comfortable transition time. At level 3, the vehicle is designed so that the driver is not expected to constantly monitor the roadway while driving.\n\n\nLevel 4 - Full Self-Driving Automation:\n The vehicle is designed to perform all safety-critical driving functions and monitor roadway conditions for an entire trip. The driver is not expected to be available for control at any time during the trip.\n\n\n\n\nChapter 2: Sensors\n\n\nAutonomous vehicles rely on several sensors to gather information. These sensors vary widely in how they work and the types of information they provide, so recent autonomous vehicles generally use most or all of them.\n\n\nLidar\n\n\n\nLidar is one of the most prominent sensors on an autonomous vehicle, with units frequently mounted a few inches above the roof of the vehicle in the center. Lidar units typically scan in all directions around the vehicle, delivering information about the postitions of cars, buildings, and everything else. Lidar data is relatively precise, fast, and reliable, and for these reasons it can be thought of as the primary sensor for an autonomous vehicle.\n\n\nLidar works by shooting a laser pulse and measuring the time it takes for the light to strike something and be reflected back. The time it takes for the light to travel is used to determine the distance to the nearest object in whichever direction it was pointing. Lidar units spin the laser and shoot pulses hundreds of thousands or millions of times each second in different directions to capture the environment in 3D. Repectable lidar units are able to determine the brightness of the object struck (by measuring the intensity of the light reflected back) to capture the environment in black-and-white, and can detect multiple strikes from one laser pulse (such as if one pulse strikes a raindrop and travels through to another object behind it), helping them to work in adverse conditions.\n\n\nBecause of the precision required of them, lidar units start at $8000 and can cost many times more. A new technology, solid-sate lidar, promises to reduce that cost (supposedly by \nnext year\n). Solid-state lidar works mostly the same, but instead of having moving parts, it changes the angle of the laser pulse by sending it through a crystal to bend the light at an electronically-adjustable angle. While this doesn't allow for full 360-degree coverage, a few units together could provide the same sensory coverage as current units for a cost many times lower.\n\n\nRadar\n\n\n\nRadar is in some ways similar to lidar. Radar works by firing pulses of radio waves in a general direction and measuring how those waves are reflected by objects they encounter. Radar tends to be much less precise than lidar, but operates more quickly and reliably, making it helpful for tasks like keeping a safe following distance. Radar also offers the advantage that it can directly measure the speed of objects (based on the wavelengths of radio wave reflections), a task which is deceptively tricky for other types of sensors.\n\n\nCameras\n\n\n\nCameras are used by autonomous vehicles to read visual information from the environment, including lane markings, traffic lights, and signs. While people have no problem interpreting this visual information, telling a computer how to interpret images in a meaningful way is an area of active research and modern techniques often require lots of computational power.\n\n\nCameras work by using a lens to focus light from a particular direction onto an array of photosensors. These photosensors measure the total amount of light that hits them (in color cameras, the total amount of each color light that hits them) and each photosensor's measurement results in one pixel of the resulting image. It is then up to complicated computer vision algorithms to make deductions from the image.\n\n\nThere are a few creative ways to use cameras. Infrared cameras can detect the heat emited by objects, helping the vehicle identify pedestrians or vehicles. Using two cameras together, side-by-side, allows the vehicle to detect how far away objects in the image are (using some intense computation). Less commonly, the lens on a camera can have its focal length quickly adjusted and the distance to an object can be deduced by which focal length makes it appear most clearly in the camera's images.\n\n\nGPS/GBAS\n\n\n\nGPS (Global Positioning System) is used to determine the position of the vehicle, more or less. GPS units rely on satellites that broadcast the position of the satellite and the current time. By the time a signal reaches the GPS unit, the time has changed slightly, and the unit uses this time change to measure its distance from the satellite; using mutliple distances and satellites, the unit can triangulate its position. Because of the interference of Earth's atmosphere with the signals (among other complications) GPS can only determine a position within about five meters, which is an unacceptable level of precision to say the least.\n\n\nGBAS (Ground-Based Augmentation System) is used to improve the accuracy of GPS (presently used in agriculture for automated harvesting). Essentially, GBAS is just GPS but with signals being broadcast from the ground as well; broadcast stations must be within a few miles of the vehicle to work but the signal suffers less interference and positions can be determined within a few centimeters. GBAS is uncommon in autonomous vehicles because of its limited range; in practice, vehicles generally combine GPS data with other measurements (such as speed and direction) to determine position more accurately.\n\n\nIMU\n\n\n\nIMU's (Inertial Measurement Units) measure how the vehicle moves without outside information. IMU's typically have several components. Accelerometers in the IMU track how the vehicle accelerates to keep track of its velocity and therefore position (given an initial position and velocity). Gyroscopes in the IMU measure how the unit rotates to help the accelerometers keep track of which direction is which.\n\n\nWhile accelerometers and gyroscopes can be fairly accurate, the uncertainty in their measurements accumulates over time. To keep the uncertainty in check, they need to continuously calibrated by comparing their outputs to absolute measurements of position or rotation repsectively. Accelerometers' position measurements are compared to GPS or other position measurements. Gyroscopes' rotation measurements are compared to the measurements of a third sensor in the IMU, the magnetometer, which measures the unit's direction relative to the Earth's magnetic field.", 
            "title": "Knowledge"
        }, 
        {
            "location": "/knowledge/#introduction", 
            "text": "This is where everything we know about self driving vehicles will be found. We're eager to have people join, learn from, and contribute to our project.", 
            "title": "Introduction"
        }, 
        {
            "location": "/knowledge/#what-are-they", 
            "text": "Autonomous vehicles (AV's), also known as self-driving cars, are vehicles that drive themselves. For now, the vehicles are generally required to be supervised by a driver, but the goal is for the cars to be able to drive themselves on their own.", 
            "title": "What are they?"
        }, 
        {
            "location": "/knowledge/#why-are-they-important", 
            "text": "Autonomous vehicles will have an impact similar to the internet in magnitude and nature. The internet allowed us to move information at a reduced cost and without human interaction. Autonomous vehicles will have the same effect on the transportation of physical goods.  More importantly, autonomous vehicles are going to save a tremendous amount of lives. Driver error is a leading cause of death, and self-driving vehicles will make the roads safer because they aren't prone to making the same types of errors.", 
            "title": "Why are they important?"
        }, 
        {
            "location": "/knowledge/#how-do-they-work", 
            "text": "Autonomous vehicles use an array of sensors, including GPS, cameras, and radar, to gather information about the world around them (more about sensors  here ). Data from these sensors is then processed by state of the art algorithms to interpret the world in a meaningful way. They then use this interpretation to make fast and reliable decisions.", 
            "title": "How do they work?"
        }, 
        {
            "location": "/knowledge/#chapter-1-levels-of-autonomy", 
            "text": "According to the  National Highway Traffic Safety Administration , there are 5 levels of autonomy (including Level 0: No Automation). We've shortened their definitions for brevity, but the full definitions can be found  here .   Level 0 - No Automation:  The driver is soley resposible for monitoring the roadway, and for safe operation of all vehicle controls.  Level 1 - Function-specific Automation:  The driver can choose to cede limited authoiry over a primary control (as in adaptive cruise control), the vehicle can automatically assume limmited authority over a primary control (as in electronic stability control), or the automated system can provide added control to aid the driver in certain normal driving or crash-imminent situations (e.g., dynamic brake support in emergencies).  Level 2 - Combined Function Automation:  The driver is still responsible for monitoring the roadway and safe opperation and is expected to be available for control at all times and on short notice. At level 2 in the specefic operation conditions for which the system is designed, an automated operating mode is enabled such that the driver is disengaged from physically operating the vehicle by having his or her hands off the steering wheel and foot off the pedal at the same time.  Level 3 - Limited Self-Driving Automation:  Vehicles at this level of automation enable the driver to cede full control of all safety-critical functions under certain traffic or environmental conditions and in those conditions to rely heavily on the vehicle to monitor for changes in those conditions requiring transition back to driver control. The driver is expected to be available for occasional control, but with sufficiently comfortable transition time. At level 3, the vehicle is designed so that the driver is not expected to constantly monitor the roadway while driving.  Level 4 - Full Self-Driving Automation:  The vehicle is designed to perform all safety-critical driving functions and monitor roadway conditions for an entire trip. The driver is not expected to be available for control at any time during the trip.", 
            "title": "Chapter 1: Levels of autonomy"
        }, 
        {
            "location": "/knowledge/#chapter-2-sensors", 
            "text": "Autonomous vehicles rely on several sensors to gather information. These sensors vary widely in how they work and the types of information they provide, so recent autonomous vehicles generally use most or all of them.", 
            "title": "Chapter 2: Sensors"
        }, 
        {
            "location": "/birdie/", 
            "text": "The Birdie\n\n\nThe Birdie is our second vehicle. It will be built starting in the Spring of 2016 by members of the DriveAI team.\n\n\nIntroduction\n\n\nAfter completing the Cockroach, we decided that our next vehicle should be larger and more robust to accomodate for our growing capabilities. The go kart frame has reached its maximum potential, and a new vehicle is in order that can expand to meet our needs is needed. Therefore, a street legal golf cart capable of speeds of 25mph with a range of at least 30 miles will be our next purchase.\nWe are currently in the process of researching different golf carts on the market with the intention to purchase one in the coming months. As soon as the vehicle is in our possession we will begin work on the autonomous drive system.", 
            "title": "Birdie"
        }, 
        {
            "location": "/birdie/#the-birdie", 
            "text": "The Birdie is our second vehicle. It will be built starting in the Spring of 2016 by members of the DriveAI team.", 
            "title": "The Birdie"
        }, 
        {
            "location": "/birdie/#introduction", 
            "text": "After completing the Cockroach, we decided that our next vehicle should be larger and more robust to accomodate for our growing capabilities. The go kart frame has reached its maximum potential, and a new vehicle is in order that can expand to meet our needs is needed. Therefore, a street legal golf cart capable of speeds of 25mph with a range of at least 30 miles will be our next purchase.\nWe are currently in the process of researching different golf carts on the market with the intention to purchase one in the coming months. As soon as the vehicle is in our possession we will begin work on the autonomous drive system.", 
            "title": "Introduction"
        }, 
        {
            "location": "/cockroach/", 
            "text": "The Cockroach\n\n\nThe \"Cockroach\" is the colloquial name for our first vehicle. It was built over the summer of 2015 by DriveAI. Below is a breakdown of the various aspects of its design.\n\n\n\nHardware\n\n\n\n\nThe Cockroach began as a \nScooterX Baja 49cc Go-Kart\n.\n\n\nThe frame has been heavily modified. We removed the seat and bolted a sheet of particle board to the frame in its place to house all of the electronics. A custom metal body was bolted to the existing frame to give a more car-like appearance and protect the internals.\n\n\nA custom \n3D-printed gas cable actuator housing\n was designed, and uses a standard size servo motor to control acceleration.\n\n\nSteering and braking were actuated using \nFirgelli Automations linear actuators\n bolted directly to the steering column and brake cable, respectively.\n\n\n\n\nElectronics\n\n\n\n\nA 12v lead acid car battery was strapped to the front of the frame to power the systems.\n\n\nThe battery was connected to a combination voltage and current meter to monitor power draw and battery life.\n\n\nThe electrical harness was turned on and off by a 20 amp circuit breaker, which also served as a safety device.\n\n\nThe main power cables supplying the terminal blocks were 8 gauge stranded copper lines.\n\n\nPower distribustion and grounding was handled using screw-type terminal blocks.\n\n\nThe visual processor and decision-making computer was an \nNvidia Jetson Tk1\n.\n\n\nAll visual data was collected from a \nStereoLabs Zed Camera\n and sent to the Jetson over USB 3.0 in real time.\n\n\nActuation was controlled by an \nArduino Mega 2560\n which communicated with the Jetson via serial over USB 2.0.\n\n\nThe actuators themselves were each powered by an [Arduino PWM Motor Shield] (http://www.robotshop.com/en/10a-dc-motor-driver-arduino-shield.html) attached to the terminal blocks.\n\n\nA wireless router is bolted to the underside of the system to allow easy wireless access to be able to ssh into the system from far away.\n\n\n12v to 5v conversion for the arduino and sensors was done by a \nDC to DC converter\n.\n\n\nNote that the actuator for the brake applied tension to the brake cable itself, not the brake pad.\n\n\n\n\nSoftware\n\n\n\n\nThe Jetson is running the custom ARM build of Ubuntu 14.04 LTS that Nvidia shipped it with.\n\n\nOur perception package uses the ZED SDK and is written primarily in (C++).\n\n\nThe Arduino actuation controller is written in their proprietary language and is compiled down into AVR.\n\n\nThe software uses the serial interface built into Linux to send commands to the Arduino.\n\n\n\n\nActuation\n\n\nActuation of the Cockroach is controlled by an arduino project that interfaces with the rest of the DriveAI-Platform and operates according to the commands passed to it via serial. \n\n\nSynchronization\n\n\nThe control section operates asynchronously from the rest of the computer, and has a variable refresh rate. By default, the controller updates at 100Hz. Serial can be sent to the controller at any time, and is collected and applied during the next cycle. At the default refresh rate, this results in at most approximately 10-12ms of latency. This is acceptably fast for driving at low speeds, and can be made much faster as needed.\n\n\nEncoding and Transmission\n\n\nEach time the controller is sent a command, it is in the form of one byte of data. We have written an encoder for said data that provides a balance between raw speed and movement precision. Each packet looks like this:\n\n\n\n\nThe first two bits of data indicate which control element is being set. This configuration allows for up to 4 individual controls, which we have assigned as follows:\n\n\n\n\n0b\n00\n - Stop the engine immediately.\n\n\n0b\n01\n - Control steering.\n\n\n0b\n10\n - Control acceleration.\n\n\n0b\n11\n - Control braking.\n\n\n\n\nThe last 6 bits of data represent the position value being written to that control. This translates to an unsigned integer from 0-63. We believe that 64 positions per control is adequate for our current vehicle. It is our intention in the future to increase the precision of these controls for use in larger vehicles. Therefore, the currently established ranges for the controls in hexadecimal are as follows:\n\n\n\n\n0x\n00\n to 0x\n3F\n - Stop the engine immediately.\n\n\n0x\n40\n to 0x\n7F\n - Control steering, with 0x\n40\n being extreme left, and 0x\n7F\n being extreme right.\n\n\n0x\n80\n to 0x\nBF\n - Control acceleration, with 0x\n80\n being full throttle, and 0x\nBF\n being zero throttle.\n\n\n0x\nc0\n to 0x\nFF\n - Control steering, with 0x\nC0\n being no braking, and 0x\nFF\n being full power braking.", 
            "title": "Cockroach"
        }, 
        {
            "location": "/cockroach/#the-cockroach", 
            "text": "The \"Cockroach\" is the colloquial name for our first vehicle. It was built over the summer of 2015 by DriveAI. Below is a breakdown of the various aspects of its design.", 
            "title": "The Cockroach"
        }, 
        {
            "location": "/cockroach/#hardware", 
            "text": "The Cockroach began as a  ScooterX Baja 49cc Go-Kart .  The frame has been heavily modified. We removed the seat and bolted a sheet of particle board to the frame in its place to house all of the electronics. A custom metal body was bolted to the existing frame to give a more car-like appearance and protect the internals.  A custom  3D-printed gas cable actuator housing  was designed, and uses a standard size servo motor to control acceleration.  Steering and braking were actuated using  Firgelli Automations linear actuators  bolted directly to the steering column and brake cable, respectively.", 
            "title": "Hardware"
        }, 
        {
            "location": "/cockroach/#electronics", 
            "text": "A 12v lead acid car battery was strapped to the front of the frame to power the systems.  The battery was connected to a combination voltage and current meter to monitor power draw and battery life.  The electrical harness was turned on and off by a 20 amp circuit breaker, which also served as a safety device.  The main power cables supplying the terminal blocks were 8 gauge stranded copper lines.  Power distribustion and grounding was handled using screw-type terminal blocks.  The visual processor and decision-making computer was an  Nvidia Jetson Tk1 .  All visual data was collected from a  StereoLabs Zed Camera  and sent to the Jetson over USB 3.0 in real time.  Actuation was controlled by an  Arduino Mega 2560  which communicated with the Jetson via serial over USB 2.0.  The actuators themselves were each powered by an [Arduino PWM Motor Shield] (http://www.robotshop.com/en/10a-dc-motor-driver-arduino-shield.html) attached to the terminal blocks.  A wireless router is bolted to the underside of the system to allow easy wireless access to be able to ssh into the system from far away.  12v to 5v conversion for the arduino and sensors was done by a  DC to DC converter . \nNote that the actuator for the brake applied tension to the brake cable itself, not the brake pad.", 
            "title": "Electronics"
        }, 
        {
            "location": "/cockroach/#software", 
            "text": "The Jetson is running the custom ARM build of Ubuntu 14.04 LTS that Nvidia shipped it with.  Our perception package uses the ZED SDK and is written primarily in (C++).  The Arduino actuation controller is written in their proprietary language and is compiled down into AVR.  The software uses the serial interface built into Linux to send commands to the Arduino.", 
            "title": "Software"
        }, 
        {
            "location": "/cockroach/#actuation", 
            "text": "Actuation of the Cockroach is controlled by an arduino project that interfaces with the rest of the DriveAI-Platform and operates according to the commands passed to it via serial.", 
            "title": "Actuation"
        }, 
        {
            "location": "/cockroach/#synchronization", 
            "text": "The control section operates asynchronously from the rest of the computer, and has a variable refresh rate. By default, the controller updates at 100Hz. Serial can be sent to the controller at any time, and is collected and applied during the next cycle. At the default refresh rate, this results in at most approximately 10-12ms of latency. This is acceptably fast for driving at low speeds, and can be made much faster as needed.", 
            "title": "Synchronization"
        }, 
        {
            "location": "/cockroach/#encoding-and-transmission", 
            "text": "Each time the controller is sent a command, it is in the form of one byte of data. We have written an encoder for said data that provides a balance between raw speed and movement precision. Each packet looks like this:   The first two bits of data indicate which control element is being set. This configuration allows for up to 4 individual controls, which we have assigned as follows:   0b 00  - Stop the engine immediately.  0b 01  - Control steering.  0b 10  - Control acceleration.  0b 11  - Control braking.   The last 6 bits of data represent the position value being written to that control. This translates to an unsigned integer from 0-63. We believe that 64 positions per control is adequate for our current vehicle. It is our intention in the future to increase the precision of these controls for use in larger vehicles. Therefore, the currently established ranges for the controls in hexadecimal are as follows:   0x 00  to 0x 3F  - Stop the engine immediately.  0x 40  to 0x 7F  - Control steering, with 0x 40  being extreme left, and 0x 7F  being extreme right.  0x 80  to 0x BF  - Control acceleration, with 0x 80  being full throttle, and 0x BF  being zero throttle.  0x c0  to 0x FF  - Control steering, with 0x C0  being no braking, and 0x FF  being full power braking.", 
            "title": "Encoding and Transmission"
        }
    ]
}